# Papers
This repo consist of all those papers I came across my course of research which I found very interesting and mind boggling.

<h2>Neural Module Networks</h2>
The paper adresses the problem statement of Visual Question and Answering using a new set of proposed networks called Neural Module Networks. The idea presented throught this paper is to use sematic parsing to get a representations of a questions posed and using some given module a layout to the network is decided using the available modules. The modules work on images particularly. There are 4 modules decribed in the paper namely find, transform, describe and. While parsing "Where is the cat" becomes where(cat) which is then replaced by find(cat). This find module then produces an attention on the cat in the image and the output is predicted as couch. An LSTM encoder is also used to preserve the informations which the module cannot keep like "is" or "are". The model is finally trained end to end on input images and questions with labels.

<h2>Two Step Disentanglement</h2>
The paper presents a idea to represent unique disentangled representations of an image. Disentangled representations means to get representation of style and content of an image. The proposed method above usesan autoencoder structure with two encoded vectors concatenated to produce the output image. The beauty of the paper is the adversarial loss they use in on of the encoded vectors which can be thought of a increasing softmax loss. Since the loss is made to increase the encoded vector tries to get encodings of the class independent information otherwise understood as style representation of the image. The model is effective in encodeing the style information as shown in results.

<h2>Wasserstien GAN</h2>
Well It is a must read paper for those interested and intrigued by idea behind GANs. I will just write a very brief and simple explanation about the intution behind the paper. Wasserstien Distance is concept in mathematics that is represented by the minimum cost/value which we need to connvert any point in one distritribution to another distribution. This could sound a lot familiar with intuition of GANs. The generator usually tries to fool the discrimiinator or vice versa. In other words the generator tries to match the distribution of the input data where as the discriminator tries to maintain the gap between input distribution and the distribution of the generated data. So the concept of  Wasserstien Distance is used to train the discriminator. Since the discriminator tries to maximize the distance between the distribution, in other words it tries to maximize the distance between two points in two different distributions so comes the intuition behind the use of Wasserstien Distance. In the actual paper WD was used to adress the vanishing gradient problem in GANs.
